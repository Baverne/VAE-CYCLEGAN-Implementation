#!/usr/bin/env python3
"""
Script pour t√©l√©charger un √©chantillon du dataset Hypersim avec diversit√© maximale.

Utilisation:
    cloner le repo ml-hypersim depuis https://github.com/apple/ml-hypersim : git clone https://github.com/apple/ml-hypersim
    installer les d√©pendances requises : pip install -r requirements.txt
    
    puis ex√©cuter ce script.
    Exemples:
    # T√©l√©charger 100 images avec depth, semantic et normal
    python download_dataset_sample.py --num_images 100 --modalities depth semantic normal --repo_path /path/to/ml-hypersim --output_dir my_dataset --seed 123
    
    # T√©l√©charger 50 images avec toutes les modalit√©s
    python download_dataset_sample.py --num_images 50 --modalities all_modalities --repo_path /path/to/ml-hypersim --output_dir my_dataset --seed 123


 
"""

import argparse
import os
import sys
import zipfile
import requests
import h5py
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import pandas as pd
from collections import defaultdict
import shutil
from tqdm import tqdm

# Augmenter la vitesse de t√©l√©chargement
zipfile.ZipExtFile.MIN_READ_SIZE = 2 ** 20

# Base URL et liste des sc√®nes disponibles
BASE_URL = "https://docs-assets.developer.apple.com/ml-research/datasets/hypersim/v1/scenes/"

# D√©finition des modalit√©s disponibles
# Format: (nom_modalit√©, nom_fichier_source, is_hdf5, r√©pertoire_source)
MODALITIES_CONFIG = [
    ('color', 'tonemap.jpg', False, 'final_preview'),
    ('depth', 'depth_meters.hdf5', True, 'geometry_hdf5'),
    ('semantic', 'semantic.hdf5', True, 'geometry_hdf5'),
    ('semantic_instance', 'semantic_instance.hdf5', True, 'geometry_hdf5'),
    ('normal', 'normal_cam.hdf5', True, 'geometry_hdf5'),
    ('normal_world', 'normal_world.hdf5', True, 'geometry_hdf5'),
    ('normal_bump', 'normal_bump_cam.hdf5', True, 'geometry_hdf5'),
    ('position', 'position.hdf5', True, 'geometry_hdf5'),
    ('render_entity_id', 'render_entity_id.hdf5', True, 'geometry_hdf5'),
]

# Construction du mapping des modalit√©s
MODALITY_MAPPINGS = {}
for modality_name, source_file, is_hdf5, source_dir in MODALITIES_CONFIG:
    pattern = f'scene_cam_{{cam}}_{source_dir}/frame.{{frame:04d}}.{source_file}'
    MODALITY_MAPPINGS[modality_name] = {
        'pattern': pattern,
        'is_hdf5': is_hdf5,
        'output_name': f'{modality_name}.png'
    }


class WebFile:
    """Fichier web avec support de lecture partielle."""
    def __init__(self, url, session):
        with session.head(url) as response:
            size = int(response.headers["content-length"])
        
        self.url = url
        self.session = session
        self.offset = 0
        self.size = size
    
    def seekable(self):
        return True
    
    def tell(self):
        return self.offset
    
    def available(self):
        return self.size - self.offset
    
    def seek(self, offset, whence=0):
        if whence == 0:
            self.offset = offset
        elif whence == 1:
            self.offset = min(self.offset + offset, self.size)
        elif whence == 2:
            self.offset = max(0, self.size + offset)
    
    def read(self, n=None):
        if n is None:
            n = self.available()
        else:
            n = min(n, self.available())
        
        end_inclusive = self.offset + n - 1
        
        headers = {
            "Range": f"bytes={self.offset}-{end_inclusive}",
        }
        
        with self.session.get(self.url, headers=headers) as response:
            data = response.content
        
        self.offset += len(data)
        
        return data


def normalize_for_display(data):
    """Normalise les donn√©es pour affichage."""
    data = np.array(data, dtype=np.float32)
    
    valid_mask = np.isfinite(data)
    if not np.any(valid_mask):
        return np.zeros_like(data)
    
    data_min = np.min(data[valid_mask])
    data_max = np.max(data[valid_mask])
    
    if data_max - data_min < 1e-10:
        return np.zeros_like(data)
    
    normalized = (data - data_min) / (data_max - data_min)
    normalized[~valid_mask] = 0
    
    return normalized


def convert_hdf5_to_png(hdf5_data, modality_name, output_path):
    """Convertit les donn√©es HDF5 en PNG."""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Traitement selon la modalit√©
    if 'semantic' in modality_name or 'render_entity_id' in modality_name:
        # Segmentation - utiliser colormap
        if len(hdf5_data.shape) == 2:
            normalized = normalize_for_display(hdf5_data)
            cmap = plt.get_cmap('tab20')
            img = cmap(normalized)[:, :, :3]
        else:
            img = normalize_for_display(hdf5_data)
    
    elif 'normal' in modality_name:
        # Normal maps - convertir de [-1, 1] √† [0, 1]
        img = (hdf5_data + 1.0) / 2.0
        img = np.clip(img, 0, 1)
    
    elif 'depth' in modality_name or 'position' in modality_name:
        # Depth/position - normaliser avec colormap
        if len(hdf5_data.shape) == 2:
            normalized = normalize_for_display(hdf5_data)
            cmap = plt.get_cmap('plasma')
            img = cmap(normalized)[:, :, :3]
        else:
            img = normalize_for_display(hdf5_data)
    
    else:
        # G√©n√©rique
        img = normalize_for_display(hdf5_data)
    
    plt.imsave(output_path, img)
    return output_path


def load_scene_metadata(repo_path=None):
    """Charge les m√©tadonn√©es des sc√®nes."""
    if repo_path is None:
        # Chemin relatif par d√©faut (script dans contrib/99991)
        metadata_path = Path('../../evermotion_dataset/analysis/metadata_camera_trajectories.csv')
    else:
        metadata_path = Path(repo_path) / 'evermotion_dataset' / 'analysis' / 'metadata_camera_trajectories.csv'
    
    if not metadata_path.exists():
        print(f"‚ö†Ô∏è  M√©tadonn√©es non trouv√©es: {metadata_path}")
        return {}
    
    df = pd.read_csv(metadata_path)
    
    # Cr√©er un mapping scene_name -> scene_type
    scene_types = {}
    for _, row in df.iterrows():
        animation = row['Animation']
        scene_name = '_'.join(animation.split('_')[:3])  # ai_001_001
        scene_type = row['Scene type']
        
        if scene_name not in scene_types:
            scene_types[scene_name] = scene_type
    
    return scene_types


def get_scene_name_with_type(scene_name, scene_types):
    """Retourne le nom de sc√®ne avec son type."""
    scene_type = scene_types.get(scene_name, 'unknown')
    # Nettoyer le type pour le nom de fichier
    scene_type_clean = scene_type.lower().replace(' ', '_').replace('(', '').replace(')', '')
    return f"{scene_name}_{scene_type_clean}"


def plan_download(num_images, seed=42, repo_path=None):
    """
    Planifie quelles images t√©l√©charger en maximisant la diversit√© de sc√®nes.
    
    Retourne une liste d√©terministe de (scene_name, camera_name, frame_id).
    """
    # Fixer le seed pour reproductibilit√©
    np.random.seed(seed)
    
    # Charger les m√©tadonn√©es
    if repo_path is None:
        # Chemin relatif par d√©faut (script dans contrib/99991)
        metadata_path = Path('../../evermotion_dataset/analysis/metadata_images.csv')
    else:
        metadata_path = Path(repo_path) / 'evermotion_dataset' / 'analysis' / 'metadata_images.csv'
    
    if not metadata_path.exists():
        raise FileNotFoundError(f"‚ö†Ô∏è  M√©tadonn√©es non trouv√©es, utilisation d'un plan par d√©faut")
    print(f"metadata_path : {metadata_path}")
    df = pd.read_csv(metadata_path)
    print("Dataframe des m√©tadonn√©es non filtr√©es:")
    print(df)
    # Ne garder que les images publiquement disponibles
    df = df[df['included_in_public_release'] == True]
    print("Dataframe des m√©tadonn√©es filtr√©es:")
    print(df)

    
    # Obtenir toutes les sc√®nes uniques
    scenes = df['scene_name'].unique()
    scenes.sort()  # Tri pour d√©terminisme
    
    # Calculer combien d'images par sc√®ne (distribution uniforme)
    images_per_scene = max(1, num_images // len(scenes))
    
    plan = []
    scene_idx = 0
    
    while len(plan) < num_images:
        for scene in scenes:
            if len(plan) >= num_images:
                break
            
            # Obtenir les cam√©ras et frames pour cette sc√®ne
            scene_data = df[df['scene_name'] == scene]
            
            if len(scene_data) == 0:
                continue
            
            # Prendre une cam√©ra (la premi√®re par ordre alphab√©tique pour d√©terminisme)
            cameras = sorted(scene_data['camera_name'].unique())
            camera = cameras[0]
            
            # Prendre des frames espac√©es uniform√©ment
            scene_camera_data = scene_data[scene_data['camera_name'] == camera]
            frames = sorted(scene_camera_data['frame_id'].unique())
            
            if len(frames) == 0:
                continue
            
            # S√©lectionner une frame (espac√©e uniform√©ment)
            frame_idx = (len(plan) // len(scenes)) % len(frames)
            frame = frames[min(frame_idx, len(frames) - 1)]
            
            plan.append((scene, camera, frame))
    
    return plan[:num_images]


def download_and_convert(session, url, scene_name, camera_name, frame_id, modalities, output_dir, scene_types, temp_dir, verbose=True):
    """T√©l√©charge et convertit en png les modalit√©s pour une image donn√©e."""
    
    scene_name_with_type = get_scene_name_with_type(scene_name, scene_types)
    
    # Cr√©er le r√©pertoire de sortie qi il n'existe pas
    output_scene_dir = output_dir / scene_name_with_type / camera_name
    if not output_scene_dir.exists():
        output_scene_dir.mkdir(parents=True, exist_ok=True)
    if verbose:
        print(f"\n T√©l√©chargement: {scene_name_with_type}/{camera_name}/frame_{frame_id:04d}")
    
    try:
        # Ouvrir le fichier ZIP distant
        f = WebFile(url, session)
        z = zipfile.ZipFile(f)
        
        downloaded_count = 0
        
        for modality in modalities:
            if modality not in MODALITY_MAPPINGS:
                print(f"  ‚ö†Ô∏è  Modalit√© inconnue: {modality}")
                continue
            
            mapping = MODALITY_MAPPINGS[modality]
            
            # Construire le chemin du fichier dans le ZIP
            cam_num = camera_name.replace('cam_', '')
            file_pattern = mapping['pattern'].format(cam=cam_num, frame=frame_id)
            file_path_in_zip = f"{scene_name}/images/{file_pattern}"
            
            try:
                # V√©rifier si le fichier existe dans le ZIP
                if file_path_in_zip not in z.namelist():
                    print(f"  ‚ö†Ô∏è  Fichier non trouv√©: {file_pattern}")
                    continue
                
                output_filename = f"frame_{frame_id:04d}_{mapping['output_name']}"
                output_path = output_scene_dir / output_filename
                
                # V√©rifier si le fichier existe d√©j√†
                if output_path.exists():
                    if verbose:
                        print(f"  ‚è≠Ô∏è  {modality}: d√©j√† t√©l√©charg√©")
                    downloaded_count += 1
                    continue
                
                if mapping['is_hdf5']:
                    # Extraire temporairement le HDF5
                    temp_hdf5 = temp_dir / f"temp_{modality}.hdf5"
                    with z.open(file_path_in_zip) as zf:
                        with open(temp_hdf5, 'wb') as tf:
                            tf.write(zf.read())
                    
                    # Lire et convertir
                    with h5py.File(temp_hdf5, 'r') as hf:
                        data = hf['dataset'][:]
                    
                    convert_hdf5_to_png(data, modality, output_path)
                    
                    # Supprimer le fichier temporaire
                    temp_hdf5.unlink()
                else:
                    # Fichier JPG/PNG - copier directement
                    with z.open(file_path_in_zip) as zf:
                        # Si c'est un JPG, le convertir en PNG
                        if file_path_in_zip.endswith('.jpg'):
                            import PIL.Image
                            img = PIL.Image.open(zf)
                            img.save(output_path)
                        else:
                            with open(output_path, 'wb') as of:
                                of.write(zf.read())
                if verbose:
                    print(f"  ‚úì {modality}: {output_filename}")
                downloaded_count += 1
                
            except Exception as e:
                print(f"  ‚úó Erreur {modality}: {e}")
                continue
        
        return downloaded_count > 0
        
    except Exception as e:
        print(f"  ‚úó Erreur lors du t√©l√©chargement: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='T√©l√©charge un √©chantillon diversifi√© du dataset Hypersim',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
    # T√©l√©charger 100 images avec depth, semantic et normal
    python download_dataset_sample.py --num_images 100 --modalities depth semantic normal --repo_path /path/to/ml-hypersim --output_dir my_dataset --seed 123
    
    # T√©l√©charger 50 images avec toutes les modalit√©s
    python download_dataset_sample.py --num_images 50 --modalities all_modalities --repo_path /path/to/ml-hypersim --output_dir my_dataset --seed 123
    
"""
    )
    
    parser.add_argument('--num_images', type=int, required=True,
                        help='Nombre d\'images √† t√©l√©charger')
    parser.add_argument('--modalities', nargs='+', required=True,
                        help='Liste des modalit√©s √† t√©l√©charger ou "all_modalities"')
    parser.add_argument('--output_dir', type=str, default='hypersim_sample',
                        help='R√©pertoire de sortie (d√©faut: hypersim_sample)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Seed pour reproductibilit√© (d√©faut: 42)')
    parser.add_argument('--repo_path', type=str, default=None,
                        help='Chemin vers le repo ml-hypersim (d√©faut: chemin relatif depuis contrib/99991)')
    
    args = parser.parse_args()
    
    # Traiter les modalit√©s
    if 'all_modalities' in args.modalities:
        modalities = list(MODALITY_MAPPINGS.keys())
    else:
        modalities = args.modalities
        # V√©rifier que toutes les modalit√©s sont valides
        invalid = [m for m in modalities if m not in MODALITY_MAPPINGS]
        if invalid:
            print(f"‚ùå Modalit√©s invalides: {invalid}")
            print(f"Modalit√©s disponibles: {list(MODALITY_MAPPINGS.keys())}")
            return 1
    
    output_dir = Path(args.output_dir)
    temp_dir = output_dir / '_temp'
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*70)
    print("T√âL√âCHARGEMENT D'UN √âCHANTILLON DU DATASET HYPERSIM")
    print("="*70)
    print(f"\nüìä Configuration:")
    print(f"  Nombre d'images: {args.num_images}")
    print(f"  Modalit√©s: {', '.join(modalities)}")
    print(f"  R√©pertoire de sortie: {output_dir}")
    print(f"  Seed: {args.seed}")
    
    # Charger les m√©tadonn√©es des sc√®nes
    print(f"\nüìö Chargement des m√©tadonn√©es...")
    scene_types = load_scene_metadata(repo_path=args.repo_path)
    print(f"  Types de sc√®nes charg√©s: {len(scene_types)}")
    
    # Planifier les t√©l√©chargements
    print(f"\nüìã Planification des t√©l√©chargements...")
    plan = plan_download(args.num_images, seed=args.seed, repo_path=args.repo_path)
    print(f"  Images planifi√©es: {len(plan)}")
    
    if len(plan) < 20:
        print("plan :")
        print(plan)
    # Grouper par sc√®ne pour optimiser les t√©l√©chargements
    scenes_to_download = defaultdict(list)
    for scene_name, camera_name, frame_id in plan:
        scenes_to_download[scene_name].append((camera_name, frame_id))
    
    print(f"  Sc√®nes diff√©rentes: {len(scenes_to_download)}")
    
    # Cr√©er une session pour r√©utiliser les connexions
    session = requests.session()
    
    # T√©l√©charger
    total_downloaded = 0
    total_failed = 0
    
    total_frames = sum(len(frames) for frames in scenes_to_download.values())
    with tqdm(total=total_frames, desc="T√©l√©chargement des images") as pbar:
        for scene_name, frames in scenes_to_download.items():
            # Trouver l'URL correspondante
            url = f"{BASE_URL}{scene_name}.zip"
            
            # T√©l√©charger toutes les frames de cette sc√®ne
            for camera_name, frame_id in frames:
                success = download_and_convert(
                    session, url, scene_name, camera_name, frame_id,
                    modalities, output_dir, scene_types, temp_dir, verbose=False
                )
                if success:
                    total_downloaded += 1
                else:
                    total_failed += 1
                pbar.update(1)
    
    # Nettoyer le r√©pertoire temporaire
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    
    # R√©sum√©
    print("\n" + "="*70)
    print("T√âL√âCHARGEMENT TERMIN√â")
    print("="*70)
    print(f"\n‚úì Images t√©l√©charg√©es avec succ√®s: {total_downloaded}/{args.num_images}")
    if total_failed > 0:
        print(f"‚úó √âchecs: {total_failed}")
    print(f"\nüìÅ R√©pertoire de sortie: {output_dir.absolute()}")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
